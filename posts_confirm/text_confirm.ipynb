{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La réponse correspond à la mission : Prendre une photo de vous avec @product\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Missions à remplir\n",
    "missions = [\n",
    "    \"Faire un post pour présenter @product\",\n",
    "    \"Faire une story à propos de @product\",\n",
    "    \"Prendre une photo de vous avec @product\"\n",
    "]\n",
    "\n",
    "# Réponse à vérifier\n",
    "response = \"J'ai acheté @product hier et je suis très satisfait de mon achat. J'ai publié une photo avec @product sur Instagram et j'ai partagé une story à propos de mon expérience avec @product.\"\n",
    "\n",
    "# Expression régulière pour extraire les tags et les mentions\n",
    "pattern = r\"(#\\w+)|(@\\w+)\"\n",
    "\n",
    "# Extraire les tags et les mentions de la réponse\n",
    "response_tags_mentions = set(re.findall(pattern, response))\n",
    "\n",
    "# Extraire le texte de la réponse\n",
    "response_text = re.sub(pattern, \"\", response).strip()\n",
    "\n",
    "# Vectoriser le texte des missions et de la réponse\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "mission_vectors = vectorizer.fit_transform(missions)\n",
    "response_vector = vectorizer.transform([response_text])\n",
    "\n",
    "# Entraîner un modèle de classification logistique pour prédire la mission appropriée\n",
    "clf = LogisticRegression(random_state=0, solver='lbfgs', multi_class='multinomial')\n",
    "clf.fit(mission_vectors, np.arange(len(missions)))\n",
    "prediction = clf.predict(response_vector)[0]\n",
    "\n",
    "# Afficher la mission prédite\n",
    "print(f\"La réponse correspond à la mission : {missions[prediction]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faire un post pour présenter\n",
      "J'ai acheté  hier et je suis très satisfait de mon achat. J'ai publié une photo avec  sur Instagram et j'ai partagé une story à propos de mon expérience avec .\n",
      "Faire une story à propos de\n",
      "J'ai acheté  hier et je suis très satisfait de mon achat. J'ai publié une photo avec  sur Instagram et j'ai partagé une story à propos de mon expérience avec .\n",
      "Prendre une photo de vous avec\n",
      "J'ai acheté  hier et je suis très satisfait de mon achat. J'ai publié une photo avec  sur Instagram et j'ai partagé une story à propos de mon expérience avec .\n",
      "Le score de validation pour la réponse est : 3/6\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Missions à remplir\n",
    "missions = [\n",
    "    \"Faire un post pour présenter @product\",\n",
    "    \"Faire une story à propos de @product\",\n",
    "    \"Prendre une photo de vous avec @product\"\n",
    "]\n",
    "\n",
    "# Réponse à vérifier\n",
    "response = \"J'ai acheté @product hier et je suis très satisfait de mon achat. J'ai publié une photo avec @product sur Instagram et j'ai partagé une story à propos de mon expérience avec @product.\"\n",
    "\n",
    "# Expression régulière pour extraire les tags et les mentions\n",
    "pattern = r\"(#\\w+)|(@\\w+)\"\n",
    "\n",
    "# Extraire les tags et les mentions de la réponse\n",
    "response_tags_mentions = set(re.findall(pattern, response))\n",
    "\n",
    "# Extraire le texte de la réponse\n",
    "response_text = re.sub(pattern, \"\", response).strip()\n",
    "\n",
    "# Initialiser le score de validation à 0\n",
    "#validation_score = 0\n",
    "\n",
    "# Vérifier chaque mission\n",
    "#for mission in missions:\n",
    "    # Extraire les tags et les mentions de la mission\n",
    "    mission_tags_mentions = set(re.findall(pattern, mission))\n",
    "    # Vérifier si tous les tags et mentions de la mission sont présents dans la réponse\n",
    "    if mission_tags_mentions.issubset(response_tags_mentions):\n",
    "        # Ajouter un point si tous les tags et mentions sont présents\n",
    "        validation_score += 1\n",
    "    # Vérifier si le texte de la mission est présent dans la réponse\n",
    "    if mission_text := re.sub(pattern, \"\", mission).strip():\n",
    "        print(mission_text)\n",
    "        print(response_text)\n",
    "        if mission_text in response_text:\n",
    "            print(\"in\")\n",
    "            # Ajouter un point si le texte de la mission est présent\n",
    "            validation_score += 1\n",
    "\n",
    "# Afficher le score de validation\n",
    "print(f\"Le score de validation pour la réponse est : {validation_score}/{len(missions)*2}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write a positive sentence about SaTT 0\n",
      "Include #SaTT #crypto #influencer #Post2Earn #SocialFi  6\n",
      "Include “Earn crypto with your social networks” in your post 7\n",
      "Faire une story à propos de @product 9\n",
      "Le score de validation pour la réponse est : 2.5/5\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import joblib\n",
    "import requests\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import re\n",
    "import numpy as np\n",
    "import torch\n",
    "import tweepy\n",
    "from decouple import config\n",
    "\n",
    "missions = [\n",
    "\"Write a positive sentence about SaTT\",\n",
    "\"Include #SaTT #crypto #influencer #Post2Earn #SocialFi \",\n",
    "\"Include “Earn crypto with your social networks” in your post \"\n",
    "]\n",
    "\n",
    "response = \"J'ai acheté @product #SaTT #crypto  hier et je suis très satisfait de mon achat.\"\n",
    "def text_confirm(mission,response):\n",
    "    # Chargement du modèle de traitement du langage naturel BERT\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "    model = AutoModel.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "    def encode_text(text):\n",
    "        tokens = tokenizer(text, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**tokens)\n",
    "            encoded_text = outputs[0][:, 0, :].numpy()\n",
    "        return encoded_text\n",
    "\n",
    "    # Expression régulière pour extraire les tags et les mentions\n",
    "    pattern = r\"([@#]\\w+)\"\n",
    "    lien_pattern= re.compile(\n",
    "            r'(https?:\\/\\/(?:www\\.|(?!www))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|https?:\\/\\/(?:www\\.|(?!www))'\n",
    "            r'[a-zA-Z0-9]\\.[^\\s]{2,}|www\\.[a-zA-Z0-9]\\.[^\\s]{2,})')\n",
    "    # Extraire les tags et les mentions de la réponse\n",
    "    response_tags_mentions = set(re.findall(pattern, response))\n",
    "    response_lien = set(re.findall(lien_pattern, response))\n",
    "    # Extraire le texte de la réponse\n",
    "    response_text = re.sub(pattern, \"\", response).strip()\n",
    "    response_text = re.sub(lien_pattern, \"\", response_text).strip()\n",
    "    # Initialiser le score de validation à 0\n",
    "    validation_score = 0\n",
    "    total=0\n",
    "    # Vérifier chaque mission\n",
    "    for mission in missions:\n",
    "        # Extraire les tags et les mentions de la mission\n",
    "        mission_tags_mentions = set(re.findall(pattern, mission))\n",
    "        total+=len(mission_tags_mentions)\n",
    "    \n",
    "        mission_lien = set(re.findall(lien_pattern, mission))\n",
    "        total+=len(mission_lien)\n",
    "    \n",
    "        # Vérifier si tous les tags et mentions de la mission sont présents dans la réponse\n",
    "        validation_score +=len(mission_tags_mentions.intersection(response_tags_mentions))\n",
    "        validation_score +=len(mission_lien.intersection(response_lien))\n",
    "        mission_text = re.sub(pattern,\"\", mission).strip()\n",
    "        mission_text = re.sub(lien_pattern,\"\", mission).strip()\n",
    "        mission_vector = encode_text(mission_text)\n",
    "        response_vector = encode_text(response_text)\n",
    "        # Calculate cosine similarity\n",
    "        similarity = np.dot(mission_vector,  response_vector.T) / (np.linalg.norm(mission_vector) * np.linalg.norm( response_vector.T))\n",
    "        total+=2\n",
    "        if similarity > 0.7:\n",
    "            # Ajouter un point si la similarité est suffisante\n",
    "            validation_score += 1\n",
    "        clf = joblib.load('postclf.pkl')\n",
    "        predicted=clf.predict(dp1)\n",
    "    # Afficher le score de validation\n",
    "    return validation_score *5 / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n",
      "1.3636363636363635\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import joblib\n",
    "\n",
    "\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "import spacy\n",
    "from torch import cosine_similarity\n",
    "\n",
    "from Twitterpreprocessor import TwitterPreprocessor\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "from nltk.stem import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "\n",
    "missions = [\n",
    "\"Write a positive sentence about SaTT\",\n",
    "\"Include #SaTT #crypto #influencer #Post2Earn #SocialFi \",\n",
    "\"Include “Earn crypto with your social networks” in your post \"\n",
    "]\n",
    "\n",
    "response = \"J'ai acheté @product #SaTT #crypto  hier et je suis très satisfait de mon achat.\"\n",
    "def porterstemmer(text):\n",
    "  text = ' '.join(ps.stem(word) for word in text.split() if word in text)\n",
    "  return text  \n",
    "\n",
    "def lemmatization (text):\n",
    "    doc = nlp(text)\n",
    "    tokens = [token.lemma_.lower() for token in doc if not token.is_stop and not token.is_punct]\n",
    "    return ' '.join(tokens)\n",
    "def encode_text(text):\n",
    "        # clean text from stop wods and punctuation\n",
    "        text=TwitterPreprocessor(str(text)).fully_preprocess().text\n",
    "        text=porterstemmer(text)\n",
    "        text=lemmatization(text)\n",
    "        text =[text]\n",
    "        vectorizer = joblib.load('vectorizer.pkl')\n",
    "        text = vectorizer.transform(text)\n",
    "        text = torch.tensor(text.toarray())\n",
    "        return text\n",
    "def text_confirm(missions,response):\n",
    "    # Expression régulière pour extraire les tags et les mentions\n",
    "    pattern = r\"([@#]\\w+)\"\n",
    "    lien_pattern= re.compile(\n",
    "            r'(https?:\\/\\/(?:www\\.|(?!www))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|https?:\\/\\/(?:www\\.|(?!www))'\n",
    "            r'[a-zA-Z0-9]\\.[^\\s]{2,}|www\\.[a-zA-Z0-9]\\.[^\\s]{2,})')\n",
    "    # Extraire les tags et les mentions de la réponse\n",
    "    response_tags_mentions = set(re.findall(pattern, response))\n",
    "    response_lien = set(re.findall(lien_pattern, response))\n",
    "    # Extraire le texte de la réponse\n",
    "    response_text = re.sub(pattern, \"\", response).strip()\n",
    "    response_text = re.sub(lien_pattern, \"\", response_text).strip()\n",
    "    # Initialiser le score de validation à 0\n",
    "    validation_score = 0\n",
    "    total=0\n",
    "    # Vérifier chaque mission\n",
    "    for mission in missions:\n",
    "        # Extraire les tags et les mentions de la mission\n",
    "        mission_tags_mentions = set(re.findall(pattern, mission))\n",
    "        total+=len(mission_tags_mentions)\n",
    "    \n",
    "        mission_lien = set(re.findall(lien_pattern, mission))\n",
    "        total+=len(mission_lien)\n",
    "    \n",
    "        # Vérifier si tous les tags et mentions de la mission sont présents dans la réponse\n",
    "        validation_score +=len(mission_tags_mentions.intersection(response_tags_mentions))\n",
    "        validation_score +=len(mission_lien.intersection(response_lien))\n",
    "        mission_text = re.sub(pattern,\"\", mission).strip()\n",
    "        mission_text = re.sub(lien_pattern,\"\", mission).strip()\n",
    "        mission_vector = encode_text(mission_text)\n",
    "        response_vector = encode_text(response_text)\n",
    "        # Print the shapes\n",
    "        \n",
    "        mission_vector = mission_vector.float()\n",
    "        response_vector = response_vector.float()\n",
    "  \n",
    "        # Calcul de la similarité cosinus\n",
    "        # similarity = cosine_similarity(mission_vector, response_vector)[0][0]\n",
    "        similarity = np.dot(mission_vector, response_vector.T) / (np.linalg.norm(mission_vector) * np.linalg.norm( response_vector.T))\n",
    "        total+=2\n",
    "        if similarity > 0.7:\n",
    "            # Ajouter un point si la similarité est suffisante\n",
    "            validation_score += 1\n",
    "    clf = joblib.load('postclf.pkl')\n",
    "    predicted=clf.predict(response_vector)\n",
    "    print(predicted)\n",
    "    validation_score += 1 if predicted[0] == 1 else 0\n",
    "    # Afficher le score de validation\n",
    "    return round(validation_score *5 / total)\n",
    "print(text_confirm(missions,response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"finish_reason\": \"stop\",\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": null,\n",
      "      \"text\": \" 4/5\"\n",
      "    }\n",
      "  ],\n",
      "  \"created\": 1684158984,\n",
      "  \"id\": \"cmpl-7GSyWKZ6gfiG4ysiZcFBwuiADKEgj\",\n",
      "  \"model\": \"text-davinci-003\",\n",
      "  \"object\": \"text_completion\",\n",
      "  \"usage\": {\n",
      "    \"completion_tokens\": 3,\n",
      "    \"prompt_tokens\": 129,\n",
      "    \"total_tokens\": 132\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import openai\n",
    "import tweepy\n",
    "from decouple import config\n",
    "\n",
    "oracle=0\n",
    "id_poste=0\n",
    "def get_poste(oracle,id_poste):\n",
    "    desc=\"\"\n",
    "    if oracle == 0:\n",
    "        consumer_key = config('TWITTER_CONSUMER_KEY')\n",
    "        consumer_secret = config('TWITTER_CONSUMER_SECRET')\n",
    "        auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "        # auth.set_access_token(access_key,access_secret)\n",
    "        api = tweepy.API(auth, wait_on_rate_limit=True)\n",
    "    elif oracle == 1:\n",
    "        oracle=0\n",
    "    return desc   \n",
    "\n",
    "\n",
    "    \n",
    "def text_confirm():\n",
    "  mission=[]\n",
    "  poste=get_poste(oracle,id_poste)\n",
    "  openai.api_key = config('OpenAI')\n",
    "  start_sequence = \"\\nA:\"\n",
    "  restart_sequence = \"\\n\\nQ: \"\n",
    "  response = openai.Completion.create(\n",
    "    model=\"text-davinci-003\",\n",
    "    prompt=\"Q:donnes un score sur 5  pour la confirmiter de ce poste a ces mission :\\nposte: {poste} \\n mission:{mission}\\nA:\",\n",
    "    temperature=0,\n",
    "    max_tokens=100,\n",
    "    top_p=1,\n",
    "    frequency_penalty=0,\n",
    "    presence_penalty=0,\n",
    "    stop=[\"\\n\"]\n",
    "  )\n",
    "  response = list(response)\n",
    "  return response[\"choices\"][0][\"text\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

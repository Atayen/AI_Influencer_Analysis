{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La réponse correspond à la mission : Prendre une photo de vous avec @product\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Missions à remplir\n",
    "missions = [\n",
    "    \"Faire un post pour présenter @product\",\n",
    "    \"Faire une story à propos de @product\",\n",
    "    \"Prendre une photo de vous avec @product\"\n",
    "]\n",
    "\n",
    "# Réponse à vérifier\n",
    "response = \"J'ai acheté @product hier et je suis très satisfait de mon achat. J'ai publié une photo avec @product sur Instagram et j'ai partagé une story à propos de mon expérience avec @product.\"\n",
    "\n",
    "# Expression régulière pour extraire les tags et les mentions\n",
    "pattern = r\"(#\\w+)|(@\\w+)\"\n",
    "\n",
    "# Extraire les tags et les mentions de la réponse\n",
    "response_tags_mentions = set(re.findall(pattern, response))\n",
    "\n",
    "# Extraire le texte de la réponse\n",
    "response_text = re.sub(pattern, \"\", response).strip()\n",
    "\n",
    "# Vectoriser le texte des missions et de la réponse\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "mission_vectors = vectorizer.fit_transform(missions)\n",
    "response_vector = vectorizer.transform([response_text])\n",
    "\n",
    "# Entraîner un modèle de classification logistique pour prédire la mission appropriée\n",
    "clf = LogisticRegression(random_state=0, solver='lbfgs', multi_class='multinomial')\n",
    "clf.fit(mission_vectors, np.arange(len(missions)))\n",
    "prediction = clf.predict(response_vector)[0]\n",
    "\n",
    "# Afficher la mission prédite\n",
    "print(f\"La réponse correspond à la mission : {missions[prediction]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faire un post pour présenter\n",
      "J'ai acheté  hier et je suis très satisfait de mon achat. J'ai publié une photo avec  sur Instagram et j'ai partagé une story à propos de mon expérience avec .\n",
      "Faire une story à propos de\n",
      "J'ai acheté  hier et je suis très satisfait de mon achat. J'ai publié une photo avec  sur Instagram et j'ai partagé une story à propos de mon expérience avec .\n",
      "Prendre une photo de vous avec\n",
      "J'ai acheté  hier et je suis très satisfait de mon achat. J'ai publié une photo avec  sur Instagram et j'ai partagé une story à propos de mon expérience avec .\n",
      "Le score de validation pour la réponse est : 3/6\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Missions à remplir\n",
    "missions = [\n",
    "    \"Faire un post pour présenter @product\",\n",
    "    \"Faire une story à propos de @product\",\n",
    "    \"Prendre une photo de vous avec @product\"\n",
    "]\n",
    "\n",
    "# Réponse à vérifier\n",
    "response = \"J'ai acheté @product hier et je suis très satisfait de mon achat. J'ai publié une photo avec @product sur Instagram et j'ai partagé une story à propos de mon expérience avec @product.\"\n",
    "\n",
    "# Expression régulière pour extraire les tags et les mentions\n",
    "pattern = r\"(#\\w+)|(@\\w+)\"\n",
    "\n",
    "# Extraire les tags et les mentions de la réponse\n",
    "response_tags_mentions = set(re.findall(pattern, response))\n",
    "\n",
    "# Extraire le texte de la réponse\n",
    "response_text = re.sub(pattern, \"\", response).strip()\n",
    "\n",
    "# Initialiser le score de validation à 0\n",
    "validation_score = 0\n",
    "\n",
    "# Vérifier chaque mission\n",
    "for mission in missions:\n",
    "    # Extraire les tags et les mentions de la mission\n",
    "    mission_tags_mentions = set(re.findall(pattern, mission))\n",
    "    # Vérifier si tous les tags et mentions de la mission sont présents dans la réponse\n",
    "    if mission_tags_mentions.issubset(response_tags_mentions):\n",
    "        # Ajouter un point si tous les tags et mentions sont présents\n",
    "        validation_score += 1\n",
    "    # Vérifier si le texte de la mission est présent dans la réponse\n",
    "    if mission_text := re.sub(pattern, \"\", mission).strip():\n",
    "        print(mission_text)\n",
    "        print(response_text)\n",
    "        if mission_text in response_text:\n",
    "            print(\"in\")\n",
    "            # Ajouter un point si le texte de la mission est présent\n",
    "            validation_score += 1\n",
    "\n",
    "# Afficher le score de validation\n",
    "print(f\"Le score de validation pour la réponse est : {validation_score}/{len(missions)*2}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write a positive sentence about SaTT 0\n",
      "Include #SaTT #crypto #influencer #Post2Earn #SocialFi  6\n",
      "Include “Earn crypto with your social networks” in your post 7\n",
      "Faire une story à propos de @product 9\n",
      "Le score de validation pour la réponse est : 2.5/5\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import requests\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import re\n",
    "import numpy as np\n",
    "import torch\n",
    "import tweepy\n",
    "from decouple import config\n",
    "\n",
    "def get_twitter_poste(id_poste,access_token,access_token_secret):\n",
    "        consumer_key = config('TWITTER_CONSUMER_KEY')\n",
    "        consumer_secret = config('TWITTER_CONSUMER_SECRET')\n",
    "        auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "        auth.set_access_token(access_token, access_token_secret)\n",
    "        api = tweepy.API(auth, wait_on_rate_limit=True)\n",
    "        tweet = api.get_status(id_poste)\n",
    "        tweet = tweet._json\n",
    "        return tweet['text']\n",
    "def get_tiktok_poste(id_poste,refrech_token):\n",
    "        getUrl = f\"https://open-api.tiktok.com/oauth/refresh_token?client_key={config('TIKTOK_KEY')}&grant_type=refresh_token&refresh_token={refrech_token}\"\n",
    "        resMedia = requests.get(getUrl)\n",
    "        accessToken = json.loads(resMedia.text)['data']['access_token']\n",
    "        api_endpoint = \"https://open.tiktokapis.com/v2/video/query/?fields=duration,title,video_description\"\n",
    "        # Set your headers\n",
    "        headers = {\n",
    "            \"Authorization\":\"Bearer \" + accessToken,\n",
    "            \"Content-Type\": \"application/json\"\n",
    "        }\n",
    "        # Set your payload (data)\n",
    "        payload = {\n",
    "            \"filters\": {\n",
    "                \"video_ids\": [\n",
    "                    id_poste,\n",
    "                    \n",
    "                ]\n",
    "            }\n",
    "        }\n",
    "        response = requests.post(api_endpoint, headers=headers, data=payload)\n",
    "        v = response.json()['data']['videos'][0]\n",
    "        return v[\"video_description\"]+\" \"+v[\"titre\"]+\" \"+v[\"duration\"]\n",
    "        \n",
    "missions = [\n",
    "\"Write a positive sentence about SaTT\",\n",
    "\"Include #SaTT #crypto #influencer #Post2Earn #SocialFi \",\n",
    "\"Include “Earn crypto with your social networks” in your post\"\n",
    "]\n",
    "# Réponse à vérifier\n",
    "response = \"J'ai acheté @product #SaTT #crypto  hier et je suis très satisfait de mon achat.\"\n",
    "def text_confirm(mission,response):\n",
    "    # Chargement du modèle de traitement du langage naturel BERT\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "    model = AutoModel.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "    def encode_text(text):\n",
    "        tokens = tokenizer(text, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**tokens)\n",
    "            encoded_text = outputs[0][:, 0, :].numpy()\n",
    "        return encoded_text\n",
    "\n",
    "    # Expression régulière pour extraire les tags et les mentions\n",
    "    pattern = r\"([@#]\\w+)\"\n",
    "    lien_pattern= re.compile(\n",
    "            r'(https?:\\/\\/(?:www\\.|(?!www))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|https?:\\/\\/(?:www\\.|(?!www))'\n",
    "            r'[a-zA-Z0-9]\\.[^\\s]{2,}|www\\.[a-zA-Z0-9]\\.[^\\s]{2,})')\n",
    "    # Extraire les tags et les mentions de la réponse\n",
    "    response_tags_mentions = set(re.findall(pattern, response))\n",
    "    response_lien = set(re.findall(lien_pattern, response))\n",
    "    # Extraire le texte de la réponse\n",
    "    response_text = re.sub(pattern, \"\", response).strip()\n",
    "    response_text = re.sub(lien_pattern, \"\", response_text).strip()\n",
    "    # Initialiser le score de validation à 0\n",
    "    validation_score = 0\n",
    "    total=0\n",
    "    # Vérifier chaque mission\n",
    "    for mission in missions:\n",
    "        # Extraire les tags et les mentions de la mission\n",
    "        mission_tags_mentions = set(re.findall(pattern, mission))\n",
    "        total+=len(mission_tags_mentions)\n",
    "    \n",
    "        mission_lien = set(re.findall(lien_pattern, mission))\n",
    "        total+=len(mission_lien)\n",
    "    \n",
    "        # Vérifier si tous les tags et mentions de la mission sont présents dans la réponse\n",
    "        validation_score +=len(mission_tags_mentions.intersection(response_tags_mentions))\n",
    "        validation_score +=len(mission_lien.intersection(response_lien))\n",
    "        mission_text = re.sub(pattern,\"\", mission).strip()\n",
    "        mission_text = re.sub(lien_pattern,\"\", mission).strip()\n",
    "        mission_vector = encode_text(mission_text)\n",
    "        response_vector = encode_text(response_text)\n",
    "        # Calculate cosine similarity\n",
    "        similarity = np.dot(mission_vector,  response_vector.T) / (np.linalg.norm(mission_vector) * np.linalg.norm( response_vector.T))\n",
    "        total+=1\n",
    "        if similarity > 0.7:\n",
    "            # Ajouter un point si la similarité est suffisante\n",
    "            validation_score += 1\n",
    "    # Afficher le score de validation\n",
    "    return validation_score *5 / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gensim'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[34], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mgensim\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodels\u001b[39;00m \u001b[39mimport\u001b[39;00m Word2Vec\n\u001b[0;32m      6\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnltk\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtokenize\u001b[39;00m \u001b[39mimport\u001b[39;00m word_tokenize\n\u001b[0;32m      7\u001b[0m \u001b[39m# twitter \u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[39m# Missions à remplir\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'gensim'"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import re\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# twitter \n",
    "# Missions à remplir\n",
    "missions = [\n",
    "\"Write a positive sentence about SaTT\",\n",
    "\"Include #SaTT #crypto #influencer #Post2Earn #SocialFi\",\n",
    "\"Include “Earn crypto with your social networks” in your post\"\n",
    "]\n",
    "\n",
    "missions = [\n",
    "    \"Faire un post pour présenter @product\",\n",
    "    \"Faire une story à propos de @product\",\n",
    "    \"Prendre une photo de vous avec @product\"\n",
    "]\n",
    "# Réponse à vérifier\n",
    "response = \"J'ai acheté @product #SaTT #crypto  hier et je suis très satisfait de mon achat. J'ai publié une photo avec @product sur Instagram et j'ai partagé une story à propos de mon expérience avec @product.\"\n",
    "\n",
    "# Chargement du modèle de traitement du langage naturel BERT\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = AutoModel.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "def encode_text(text):\n",
    "    tokens = tokenizer(text, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**tokens)\n",
    "        encoded_text = outputs[0][:, 0, :].numpy()\n",
    "    return encoded_text\n",
    "\n",
    "# Expression régulière pour extraire les tags et les mentions\n",
    "pattern = r\"([@#]\\w+)\"\n",
    "\n",
    "# Extraire les tags et les mentions de la réponse\n",
    "response_tags_mentions = set(re.findall(pattern, response))\n",
    "\n",
    "# Extraire le texte de la réponse\n",
    "response_text = re.sub(pattern, \"\", response).strip()\n",
    "\n",
    "# Initialiser le score de validation à 0\n",
    "validation_score = 0\n",
    "total=0\n",
    "# Vérifier chaque mission\n",
    "for mission in missions:\n",
    "    # Extraire les tags et les mentions de la mission\n",
    "    mission_tags_mentions = set(re.findall(pattern, mission))\n",
    "    total+=len(mission_tags_mentions)\n",
    "    # Vérifier si tous les tags et mentions de la mission sont présents dans la réponse\n",
    "    validation_score +=len(mission_tags_mentions.intersection(response_tags_mentions))\n",
    "    # if mission_tags_mentions and response_tags_mentions.issubset(mission_tags_mentions):\n",
    "    #     print(\"in\")\n",
    "    #     # Ajouter un point si tous les tags et mentions sont présents\n",
    "    #     validation_score += 1\n",
    "    mission_text = re.sub(pattern, \"\", mission).strip()\n",
    "    mission_vector = encode_text(mission_text)\n",
    "    response_vector = encode_text(response_text)\n",
    "    # similarity = np.dot(mission_vector, response_vector.T)\n",
    "    # print(similarity)\n",
    "    # Calculate cosine similarity\n",
    "    similarity = np.dot(mission_vector,  response_vector.T) / (np.linalg.norm(mission_vector) * np.linalg.norm( response_vector.T))\n",
    "    total+=1\n",
    "    print(similarity)\n",
    "    if similarity > 0.7:\n",
    "        # Ajouter un point si la similarité est suffisante\n",
    "        validation_score += 1\n",
    "# Afficher le score de validation\n",
    "print(f\"Le score de validation pour la réponse est : {validation_score}/{total}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"finish_reason\": \"stop\",\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": null,\n",
      "      \"text\": \" 4/5\"\n",
      "    }\n",
      "  ],\n",
      "  \"created\": 1684158984,\n",
      "  \"id\": \"cmpl-7GSyWKZ6gfiG4ysiZcFBwuiADKEgj\",\n",
      "  \"model\": \"text-davinci-003\",\n",
      "  \"object\": \"text_completion\",\n",
      "  \"usage\": {\n",
      "    \"completion_tokens\": 3,\n",
      "    \"prompt_tokens\": 129,\n",
      "    \"total_tokens\": 132\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import openai\n",
    "import tweepy\n",
    "from decouple import config\n",
    "\n",
    "oracle=0\n",
    "id_poste=0\n",
    "def get_poste(oracle,id_poste):\n",
    "    desc=\"\"\n",
    "    if oracle == 0:\n",
    "        consumer_key = config('TWITTER_CONSUMER_KEY')\n",
    "        consumer_secret = config('TWITTER_CONSUMER_SECRET')\n",
    "        auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "        # auth.set_access_token(access_key,access_secret)\n",
    "        api = tweepy.API(auth, wait_on_rate_limit=True)\n",
    "    elif oracle == 1:\n",
    "        oracle=0\n",
    "    return desc   \n",
    "\n",
    "\n",
    "    \n",
    "def text_confirm():\n",
    "  mission=[]\n",
    "  poste=get_poste(oracle,id_poste)\n",
    "  openai.api_key = config('OpenAI')\n",
    "  start_sequence = \"\\nA:\"\n",
    "  restart_sequence = \"\\n\\nQ: \"\n",
    "  response = openai.Completion.create(\n",
    "    model=\"text-davinci-003\",\n",
    "    prompt=\"Q:donnes un score sur 5  pour la confirmiter de ce poste a ces mission :\\nposte: {poste} \\n mission:{mission}\\nA:\",\n",
    "    temperature=0,\n",
    "    max_tokens=100,\n",
    "    top_p=1,\n",
    "    frequency_penalty=0,\n",
    "    presence_penalty=0,\n",
    "    stop=[\"\\n\"]\n",
    "  )\n",
    "  response = list(response)\n",
    "  return response[\"choices\"][0][\"text\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "import emoji\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "from Twitterpreprocessor import TwitterPreprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Chams\\AppData\\Local\\Temp\\ipykernel_18996\\1069478916.py:1: DtypeWarning: Columns (0,3,4,14,15) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data = pd.read_csv(\"./Data/datafromAPI.csv\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>image</th>\n",
       "      <th>background_image</th>\n",
       "      <th>verified</th>\n",
       "      <th>statuses</th>\n",
       "      <th>screen_name</th>\n",
       "      <th>display_name</th>\n",
       "      <th>description</th>\n",
       "      <th>location</th>\n",
       "      <th>url</th>\n",
       "      <th>...</th>\n",
       "      <th>likes</th>\n",
       "      <th>lists</th>\n",
       "      <th>twitter_user_id</th>\n",
       "      <th>tweet_language</th>\n",
       "      <th>tweets_this_week</th>\n",
       "      <th>retweet_ratio</th>\n",
       "      <th>retweeted_count</th>\n",
       "      <th>tweets_by_day_of_week</th>\n",
       "      <th>tweets_by_hour_of_day</th>\n",
       "      <th>account_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>https://pbs.twimg.com/profile_images/145899544...</td>\n",
       "      <td>http://abs.twimg.com/images/themes/theme11/bg.gif</td>\n",
       "      <td>False</td>\n",
       "      <td>7098</td>\n",
       "      <td>paty_castroo</td>\n",
       "      <td>Patr√≠cia Castro</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Maring√°, Brasil</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>12213</td>\n",
       "      <td>2</td>\n",
       "      <td>53779179</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>bot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>https://pbs.twimg.com/profile_images/632916759...</td>\n",
       "      <td>http://abs.twimg.com/images/themes/theme3/bg.gif</td>\n",
       "      <td>False</td>\n",
       "      <td>270</td>\n",
       "      <td>CBirckner</td>\n",
       "      <td>Carli Birckner</td>\n",
       "      <td>Television producer. Emmy Award winner. Disney...</td>\n",
       "      <td>Baltimore, MD</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1484</td>\n",
       "      <td>2</td>\n",
       "      <td>105916557</td>\n",
       "      <td>en</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>103418.0</td>\n",
       "      <td>[9, 4, 12, 15, 5, 0, 3]</td>\n",
       "      <td>[1, 4, 5, 0, 1, 0, 1, 1, 0, 0, 4, 0, 0, 0, 3, ...</td>\n",
       "      <td>human</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>https://pbs.twimg.com/profile_images/158696080...</td>\n",
       "      <td>http://abs.twimg.com/images/themes/theme1/bg.png</td>\n",
       "      <td>False</td>\n",
       "      <td>14302</td>\n",
       "      <td>amf_jay</td>\n",
       "      <td>JD</td>\n",
       "      <td>self discovery üí° üß†</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2958</td>\n",
       "      <td>2</td>\n",
       "      <td>509788597</td>\n",
       "      <td>en</td>\n",
       "      <td>0.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>867392.0</td>\n",
       "      <td>[52, 14, 19, 24, 24, 29, 32]</td>\n",
       "      <td>[25, 15, 9, 20, 6, 10, 2, 9, 1, 1, 3, 0, 3, 1,...</td>\n",
       "      <td>human</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>https://pbs.twimg.com/profile_images/977012905...</td>\n",
       "      <td>http://abs.twimg.com/images/themes/theme6/bg.gif</td>\n",
       "      <td>False</td>\n",
       "      <td>4585</td>\n",
       "      <td>SaraCavolo</td>\n",
       "      <td>Sara Cavolo</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Brooklyn, NY</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>7983</td>\n",
       "      <td>3</td>\n",
       "      <td>70601327</td>\n",
       "      <td>en</td>\n",
       "      <td>0.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>2996382.0</td>\n",
       "      <td>[19, 34, 42, 41, 28, 17, 15]</td>\n",
       "      <td>[10, 17, 18, 5, 1, 0, 0, 0, 1, 0, 1, 2, 7, 18,...</td>\n",
       "      <td>human</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>https://pbs.twimg.com/profile_images/668449819...</td>\n",
       "      <td>http://abs.twimg.com/images/themes/theme14/bg.gif</td>\n",
       "      <td>True</td>\n",
       "      <td>15851</td>\n",
       "      <td>DavidHenaoModel</td>\n",
       "      <td>David Henao</td>\n",
       "      <td>Productor de Televisi√≥n - Embajador de @Tienda...</td>\n",
       "      <td>Miami, FL</td>\n",
       "      <td>https://t.co/KPAyvqMibx</td>\n",
       "      <td>...</td>\n",
       "      <td>20105</td>\n",
       "      <td>172</td>\n",
       "      <td>108999927</td>\n",
       "      <td>und</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>66477.0</td>\n",
       "      <td>[33, 23, 25, 24, 34, 28, 33]</td>\n",
       "      <td>[7, 17, 5, 3, 1, 0, 0, 2, 0, 0, 2, 6, 14, 14, ...</td>\n",
       "      <td>human</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34049</th>\n",
       "      <td>34042</td>\n",
       "      <td>https://pbs.twimg.com/profile_images/712018970...</td>\n",
       "      <td>http://abs.twimg.com/images/themes/theme14/bg.gif</td>\n",
       "      <td>True</td>\n",
       "      <td>60100</td>\n",
       "      <td>AshAlexiss</td>\n",
       "      <td>Ashley Alexiss</td>\n",
       "      <td>Self Made Plus Model, #BeautyIsNotASize, CEO @...</td>\n",
       "      <td>Los Angeles, CA</td>\n",
       "      <td>https://t.co/7nNQF782OV</td>\n",
       "      <td>...</td>\n",
       "      <td>61532</td>\n",
       "      <td>876</td>\n",
       "      <td>223764929</td>\n",
       "      <td>en</td>\n",
       "      <td>8.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>1375535.0</td>\n",
       "      <td>[28, 33, 25, 31, 22, 28, 33]</td>\n",
       "      <td>[7, 18, 13, 13, 16, 9, 6, 12, 6, 5, 1, 0, 0, 1...</td>\n",
       "      <td>human</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34050</th>\n",
       "      <td>34043</td>\n",
       "      <td>https://pbs.twimg.com/profile_images/141825399...</td>\n",
       "      <td>http://abs.twimg.com/images/themes/theme9/bg.gif</td>\n",
       "      <td>True</td>\n",
       "      <td>40277</td>\n",
       "      <td>ahmemis</td>\n",
       "      <td>Ahmet Memi≈ü</td>\n",
       "      <td>Gazeteci/Journalist</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://t.co/0wll8Iymf4</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>182</td>\n",
       "      <td>222789645</td>\n",
       "      <td>tr</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15350.0</td>\n",
       "      <td>[21, 67, 36, 18, 22, 18, 18]</td>\n",
       "      <td>[5, 0, 0, 0, 0, 1, 0, 2, 1, 6, 7, 9, 3, 7, 4, ...</td>\n",
       "      <td>human</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34051</th>\n",
       "      <td>34044</td>\n",
       "      <td>https://pbs.twimg.com/profile_images/142309872...</td>\n",
       "      <td>http://abs.twimg.com/images/themes/theme10/bg.gif</td>\n",
       "      <td>False</td>\n",
       "      <td>6679</td>\n",
       "      <td>Yuvannamontalvo</td>\n",
       "      <td>Yuvanna Montalvo</td>\n",
       "      <td>Modelo y Actriz Venezolana/ Venezuelan Model &amp;...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>28</td>\n",
       "      <td>372</td>\n",
       "      <td>80438218</td>\n",
       "      <td>zxx</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>18899.0</td>\n",
       "      <td>[25, 23, 26, 42, 29, 31, 24]</td>\n",
       "      <td>[23, 26, 21, 8, 1, 2, 0, 0, 0, 2, 0, 0, 4, 10,...</td>\n",
       "      <td>human</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34052</th>\n",
       "      <td>34045</td>\n",
       "      <td>https://pbs.twimg.com/profile_images/157735786...</td>\n",
       "      <td>http://abs.twimg.com/images/themes/theme1/bg.png</td>\n",
       "      <td>True</td>\n",
       "      <td>2580</td>\n",
       "      <td>IAmVarunTej</td>\n",
       "      <td>Varun Tej Konidela</td>\n",
       "      <td>Indian!...Actor!...Need no more!</td>\n",
       "      <td>Hyderabad</td>\n",
       "      <td>https://t.co/kJoeCCukod</td>\n",
       "      <td>...</td>\n",
       "      <td>871</td>\n",
       "      <td>378</td>\n",
       "      <td>1296642259</td>\n",
       "      <td>en</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>145792.0</td>\n",
       "      <td>[23, 22, 20, 85, 20, 14, 16]</td>\n",
       "      <td>[0, 2, 1, 26, 25, 17, 9, 7, 7, 7, 12, 5, 24, 6...</td>\n",
       "      <td>human</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34053</th>\n",
       "      <td>34046</td>\n",
       "      <td>https://pbs.twimg.com/profile_images/160574287...</td>\n",
       "      <td>http://abs.twimg.com/images/themes/theme11/bg.gif</td>\n",
       "      <td>False</td>\n",
       "      <td>3566</td>\n",
       "      <td>tiagopoeta</td>\n",
       "      <td>Tiago Poeta</td>\n",
       "      <td>üìñPoeta, Autor, Escritor, Acad√™mico. Dois Livro...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://t.co/Wu7BylmZN9</td>\n",
       "      <td>...</td>\n",
       "      <td>953</td>\n",
       "      <td>25</td>\n",
       "      <td>602979254</td>\n",
       "      <td>pt</td>\n",
       "      <td>14.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>1771.0</td>\n",
       "      <td>[18, 35, 20, 12, 33, 54, 24]</td>\n",
       "      <td>[6, 4, 16, 1, 0, 0, 0, 0, 1, 1, 6, 4, 3, 18, 1...</td>\n",
       "      <td>human</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>34054 rows √ó 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0                                              image  \\\n",
       "0              0  https://pbs.twimg.com/profile_images/145899544...   \n",
       "1              1  https://pbs.twimg.com/profile_images/632916759...   \n",
       "2              2  https://pbs.twimg.com/profile_images/158696080...   \n",
       "3              3  https://pbs.twimg.com/profile_images/977012905...   \n",
       "4              4  https://pbs.twimg.com/profile_images/668449819...   \n",
       "...          ...                                                ...   \n",
       "34049      34042  https://pbs.twimg.com/profile_images/712018970...   \n",
       "34050      34043  https://pbs.twimg.com/profile_images/141825399...   \n",
       "34051      34044  https://pbs.twimg.com/profile_images/142309872...   \n",
       "34052      34045  https://pbs.twimg.com/profile_images/157735786...   \n",
       "34053      34046  https://pbs.twimg.com/profile_images/160574287...   \n",
       "\n",
       "                                        background_image verified statuses  \\\n",
       "0      http://abs.twimg.com/images/themes/theme11/bg.gif    False     7098   \n",
       "1       http://abs.twimg.com/images/themes/theme3/bg.gif    False      270   \n",
       "2       http://abs.twimg.com/images/themes/theme1/bg.png    False    14302   \n",
       "3       http://abs.twimg.com/images/themes/theme6/bg.gif    False     4585   \n",
       "4      http://abs.twimg.com/images/themes/theme14/bg.gif     True    15851   \n",
       "...                                                  ...      ...      ...   \n",
       "34049  http://abs.twimg.com/images/themes/theme14/bg.gif     True    60100   \n",
       "34050   http://abs.twimg.com/images/themes/theme9/bg.gif     True    40277   \n",
       "34051  http://abs.twimg.com/images/themes/theme10/bg.gif    False     6679   \n",
       "34052   http://abs.twimg.com/images/themes/theme1/bg.png     True     2580   \n",
       "34053  http://abs.twimg.com/images/themes/theme11/bg.gif    False     3566   \n",
       "\n",
       "           screen_name        display_name  \\\n",
       "0         paty_castroo     Patr√≠cia Castro   \n",
       "1            CBirckner      Carli Birckner   \n",
       "2              amf_jay                  JD   \n",
       "3           SaraCavolo         Sara Cavolo   \n",
       "4      DavidHenaoModel         David Henao   \n",
       "...                ...                 ...   \n",
       "34049       AshAlexiss      Ashley Alexiss   \n",
       "34050          ahmemis         Ahmet Memi≈ü   \n",
       "34051  Yuvannamontalvo    Yuvanna Montalvo   \n",
       "34052      IAmVarunTej  Varun Tej Konidela   \n",
       "34053       tiagopoeta         Tiago Poeta   \n",
       "\n",
       "                                             description         location  \\\n",
       "0                                                    NaN  Maring√°, Brasil   \n",
       "1      Television producer. Emmy Award winner. Disney...    Baltimore, MD   \n",
       "2                                     self discovery üí° üß†              NaN   \n",
       "3                                                    NaN     Brooklyn, NY   \n",
       "4      Productor de Televisi√≥n - Embajador de @Tienda...        Miami, FL   \n",
       "...                                                  ...              ...   \n",
       "34049  Self Made Plus Model, #BeautyIsNotASize, CEO @...  Los Angeles, CA   \n",
       "34050                                Gazeteci/Journalist              NaN   \n",
       "34051  Modelo y Actriz Venezolana/ Venezuelan Model &...              NaN   \n",
       "34052                   Indian!...Actor!...Need no more!        Hyderabad   \n",
       "34053  üìñPoeta, Autor, Escritor, Acad√™mico. Dois Livro...              NaN   \n",
       "\n",
       "                           url  ...  likes lists  twitter_user_id  \\\n",
       "0                          NaN  ...  12213     2         53779179   \n",
       "1                          NaN  ...   1484     2        105916557   \n",
       "2                          NaN  ...   2958     2        509788597   \n",
       "3                          NaN  ...   7983     3         70601327   \n",
       "4      https://t.co/KPAyvqMibx  ...  20105   172        108999927   \n",
       "...                        ...  ...    ...   ...              ...   \n",
       "34049  https://t.co/7nNQF782OV  ...  61532   876        223764929   \n",
       "34050  https://t.co/0wll8Iymf4  ...      1   182        222789645   \n",
       "34051                      NaN  ...     28   372         80438218   \n",
       "34052  https://t.co/kJoeCCukod  ...    871   378       1296642259   \n",
       "34053  https://t.co/Wu7BylmZN9  ...    953    25        602979254   \n",
       "\n",
       "       tweet_language tweets_this_week retweet_ratio retweeted_count  \\\n",
       "0                 NaN              0.0           0.0             0.0   \n",
       "1                  en              0.0          16.0        103418.0   \n",
       "2                  en              0.0          77.0        867392.0   \n",
       "3                  en              0.0          54.0       2996382.0   \n",
       "4                 und              0.0          10.0         66477.0   \n",
       "...               ...              ...           ...             ...   \n",
       "34049              en              8.0          47.0       1375535.0   \n",
       "34050              tr              0.0           0.0         15350.0   \n",
       "34051             zxx              0.0           8.0         18899.0   \n",
       "34052              en              1.0           4.0        145792.0   \n",
       "34053              pt             14.0          52.0          1771.0   \n",
       "\n",
       "              tweets_by_day_of_week  \\\n",
       "0             [0, 0, 0, 0, 0, 0, 0]   \n",
       "1           [9, 4, 12, 15, 5, 0, 3]   \n",
       "2      [52, 14, 19, 24, 24, 29, 32]   \n",
       "3      [19, 34, 42, 41, 28, 17, 15]   \n",
       "4      [33, 23, 25, 24, 34, 28, 33]   \n",
       "...                             ...   \n",
       "34049  [28, 33, 25, 31, 22, 28, 33]   \n",
       "34050  [21, 67, 36, 18, 22, 18, 18]   \n",
       "34051  [25, 23, 26, 42, 29, 31, 24]   \n",
       "34052  [23, 22, 20, 85, 20, 14, 16]   \n",
       "34053  [18, 35, 20, 12, 33, 54, 24]   \n",
       "\n",
       "                                   tweets_by_hour_of_day  account_type  \n",
       "0      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...           bot  \n",
       "1      [1, 4, 5, 0, 1, 0, 1, 1, 0, 0, 4, 0, 0, 0, 3, ...         human  \n",
       "2      [25, 15, 9, 20, 6, 10, 2, 9, 1, 1, 3, 0, 3, 1,...         human  \n",
       "3      [10, 17, 18, 5, 1, 0, 0, 0, 1, 0, 1, 2, 7, 18,...         human  \n",
       "4      [7, 17, 5, 3, 1, 0, 0, 2, 0, 0, 2, 6, 14, 14, ...         human  \n",
       "...                                                  ...           ...  \n",
       "34049  [7, 18, 13, 13, 16, 9, 6, 12, 6, 5, 1, 0, 0, 1...         human  \n",
       "34050  [5, 0, 0, 0, 0, 1, 0, 2, 1, 6, 7, 9, 3, 7, 4, ...         human  \n",
       "34051  [23, 26, 21, 8, 1, 2, 0, 0, 0, 2, 0, 0, 4, 10,...         human  \n",
       "34052  [0, 2, 1, 26, 25, 17, 9, 7, 7, 7, 12, 5, 24, 6...         human  \n",
       "34053  [6, 4, 16, 1, 0, 0, 0, 0, 1, 1, 6, 4, 3, 18, 1...         human  \n",
       "\n",
       "[34054 rows x 24 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"./Data/datafromAPI.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "profile_description  am bot that tweets about sports and technology follow me for updates\n",
      "doc  am bot that tweets about sports and technology follow me for updates\n",
      "[' ', 'bot', 'tweet', 'sport', 'technology', 'follow', 'update']\n",
      "pos count Counter({'NOUN': 4, 'PRON': 2, 'VERB': 2, 'ADP': 2, 'SPACE': 1, 'AUX': 1, 'CCONJ': 1})\n",
      "Profile description: I am a bot that tweets about sports and technology. Follow me for updates!\n",
      "Is bot? False\n"
     ]
    }
   ],
   "source": [
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "stop_words = set(stopwords.words('english'))\n",
    "def is_bot_profile(profile_description):\n",
    "    # remove emojis\n",
    "    profile_description = TwitterPreprocessor(str(profile_description)).desc_preprocess().text\n",
    "    print(\"profile_description\",profile_description)\n",
    "    # tokenize the text with spaCy\n",
    "    doc = nlp(profile_description)\n",
    "    print(\"doc\",doc)\n",
    "    # remove stop words and punctuation\n",
    "    tokens = [token.lemma_ for token in doc if not token.is_punct and not token.is_stop and token.lemma_ not in stop_words]\n",
    "    print(tokens)\n",
    "    # count the frequency of different parts-of-speech\n",
    "    pos_tags = [token.pos_ for token in doc]\n",
    "    pos_counts = Counter(pos_tags)\n",
    "    print(\"pos count\",pos_counts)\n",
    "    # look for patterns typical of bot accounts\n",
    "    if pos_counts['PRON'] >= 3 and pos_counts['NOUN'] >= 3:\n",
    "        return True\n",
    "    elif pos_counts['VERB'] >= 2 and pos_counts['ADJ'] >= 2:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "#  \"I am a bot that tweets about sports and technology. Follow me for updates!\"\n",
    "profile_description =\"I am a bot that tweets about sports and technology. Follow me for updates!\"\n",
    "is_bot = is_bot_profile(profile_description)\n",
    "print(f\"Profile description: {profile_description}\")\n",
    "print(f\"Is bot? {is_bot}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i am a bot that tweets about sports and technology. follow me for updates!\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'spacy.morphology.Morphology' object has no attribute 'tag_map'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 29\u001b[0m\n\u001b[0;32m     27\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m     28\u001b[0m profile_description \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mI am a bot that tweets about sports and technology. Follow me for updates!\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m---> 29\u001b[0m is_bot \u001b[39m=\u001b[39m is_bot_profile(profile_description)\n\u001b[0;32m     30\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mProfile description: \u001b[39m\u001b[39m{\u001b[39;00mprofile_description\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     31\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mIs bot? \u001b[39m\u001b[39m{\u001b[39;00mis_bot\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[1], line 19\u001b[0m, in \u001b[0;36mis_bot_profile\u001b[1;34m(profile_description)\u001b[0m\n\u001b[0;32m     17\u001b[0m tokens \u001b[39m=\u001b[39m [token\u001b[39m.\u001b[39mlemma_ \u001b[39mfor\u001b[39;00m token \u001b[39min\u001b[39;00m doc \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m token\u001b[39m.\u001b[39mis_punct \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m token\u001b[39m.\u001b[39mis_stop \u001b[39mand\u001b[39;00m token\u001b[39m.\u001b[39mlemma_ \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m stop_words]\n\u001b[0;32m     18\u001b[0m \u001b[39m# count the frequency of different parts-of-speech\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m pos_tags \u001b[39m=\u001b[39m [nlp\u001b[39m.\u001b[39mvocab\u001b[39m.\u001b[39mmorphology\u001b[39m.\u001b[39mtag_map[token\u001b[39m.\u001b[39mtag_][\u001b[39m'\u001b[39m\u001b[39mPOS\u001b[39m\u001b[39m'\u001b[39m] \u001b[39mfor\u001b[39;00m token \u001b[39min\u001b[39;00m nlp(\u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(tokens))]\n\u001b[0;32m     20\u001b[0m pos_counts \u001b[39m=\u001b[39m Counter(pos_tags)\n\u001b[0;32m     21\u001b[0m \u001b[39m# look for patterns typical of bot accounts\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[1], line 19\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     17\u001b[0m tokens \u001b[39m=\u001b[39m [token\u001b[39m.\u001b[39mlemma_ \u001b[39mfor\u001b[39;00m token \u001b[39min\u001b[39;00m doc \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m token\u001b[39m.\u001b[39mis_punct \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m token\u001b[39m.\u001b[39mis_stop \u001b[39mand\u001b[39;00m token\u001b[39m.\u001b[39mlemma_ \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m stop_words]\n\u001b[0;32m     18\u001b[0m \u001b[39m# count the frequency of different parts-of-speech\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m pos_tags \u001b[39m=\u001b[39m [nlp\u001b[39m.\u001b[39;49mvocab\u001b[39m.\u001b[39;49mmorphology\u001b[39m.\u001b[39;49mtag_map[token\u001b[39m.\u001b[39mtag_][\u001b[39m'\u001b[39m\u001b[39mPOS\u001b[39m\u001b[39m'\u001b[39m] \u001b[39mfor\u001b[39;00m token \u001b[39min\u001b[39;00m nlp(\u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(tokens))]\n\u001b[0;32m     20\u001b[0m pos_counts \u001b[39m=\u001b[39m Counter(pos_tags)\n\u001b[0;32m     21\u001b[0m \u001b[39m# look for patterns typical of bot accounts\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'spacy.morphology.Morphology' object has no attribute 'tag_map'"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import emoji\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def is_bot_profile(profile_description):\n",
    "    # remove emojis\n",
    "    # profile_description = emoji.get_emoji_regexp().sub(r'', profile_description)\n",
    "    # lowercase the text\n",
    "    profile_description = profile_description.lower()\n",
    "    # tokenize the text with spaCy\n",
    "    doc = nlp(profile_description)\n",
    "    print(doc)\n",
    "    # remove stop words and punctuation\n",
    "    tokens = [token.lemma_ for token in doc if not token.is_punct and not token.is_stop and token.lemma_ not in stop_words]\n",
    "    print(tokens)\n",
    "    # count the frequency of different parts-of-speech\n",
    "    pos_tags = [token.pos_ for token in doc]\n",
    "    print(pos_tags)\n",
    "    pos_counts = Counter(pos_tags)\n",
    "    # look for patterns typical of bot accounts\n",
    "    if pos_counts['PRON'] >= 3 and pos_counts['NOUN'] >= 3:\n",
    "        return True\n",
    "    elif pos_counts['VERB'] >= 2 and pos_counts['ADJ'] >= 2:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "profile_description = \"I am a bot that tweets about sports and technology. Follow me for updates!\"\n",
    "is_bot = is_bot_profile(profile_description)\n",
    "print(f\"Profile description: {profile_description}\")\n",
    "print(f\"Is bot? {is_bot}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Chams\\AppData\\Local\\Temp\\ipykernel_16676\\774891700.py:7: DtypeWarning: Columns (0,3,4,14,15) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  dataset = pd.read_csv(\"./Data/datafromAPI.csv\")\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import spacy\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Load the dataset\n",
    "dataset = pd.read_csv(\"./Data/datafromAPI.csv\")\n",
    "\n",
    "# Clean the text data\n",
    "dataset['description'] = dataset['description'].apply(lambda x :TwitterPreprocessor(str(x)).desc_preprocess().text)\n",
    "dataset['description'] = dataset['description'].fillna('')\n",
    "# Define function to lemmatize the tokens\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "def lemmatize_tokens(tokens):\n",
    "    doc = nlp(\" \".join(tokens))\n",
    "    return [token.lemma_ for token in doc]\n",
    "\n",
    "# Tokenize and lemmatize the text data\n",
    "dataset['description_tokens'] = dataset['description'].apply(lambda x: lemmatize_tokens(word_tokenize(x)))\n",
    "\n",
    "# Remove stop words\n",
    "stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "dataset['description_tokens'] = dataset['description_tokens'].apply(lambda x: [word for word in x if word not in stop_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.to_csv('./Data/description_tokens.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Convert the list of tokens to a string\n",
    "dataset['description_tokens'] = dataset['description_tokens'].apply(lambda x: ' '.join(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the dense array to the dataset\n",
    "dataset['description_tokens'] = X_array.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.to_csv('./Data/description_tokensv2.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "start prepare desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv('./Data/data+descvf.csv')\n",
    "\n",
    "# Remove irrelevant or duplicated data\n",
    "df = df.drop(['description',  \n",
    "         'account_type'], axis=1)\n",
    "\n",
    "# Remove special characters and punctuation, and convert text to lowercase\n",
    "df['description'] = df['description'].apply(lambda x :TwitterPreprocessor(str(x)).desc_preprocess().text)\n",
    "\n",
    "# Tokenize text\n",
    "df['description'] = df['description'].apply(lambda x: x.split())\n",
    "\n",
    "# Remove stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "df['description'] = df['description'].apply(lambda x: [word for word in x if word not in stop_words])\n",
    "\n",
    "# Stemming\n",
    "stemmer = PorterStemmer()\n",
    "df['description'] = df['description'].apply(lambda x: [stemmer.stem(word) for word in x])\n",
    "\n",
    "# Convert back to string\n",
    "df['description'] = df['description'].apply(lambda x: ' '.join(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.models import Sequential\n",
    "# Split into training and testing sets\n",
    "train_size = int(0.8 * len(df))\n",
    "train_data = df[:train_size]\n",
    "test_data = df[train_size:]\n",
    "\n",
    "# Tokenize text inputs\n",
    "tokenizer = Tokenizer(num_words=10000, oov_token='<OOV>')\n",
    "tokenizer.fit_on_texts(train_data['description'])\n",
    "\n",
    "# Convert text inputs to sequences of integers\n",
    "train_sequences = tokenizer.texts_to_sequences(train_data['description'])\n",
    "test_sequences = tokenizer.texts_to_sequences(test_data['description'])\n",
    "\n",
    "# Pad sequences\n",
    "max_len = 100\n",
    "train_data = pad_sequences(train_sequences, maxlen=max_len, padding='post', truncating='post')\n",
    "test_data = pad_sequences(test_sequences, maxlen=max_len, padding='post', truncating='post')\n",
    "\n",
    "# Build model\n",
    "model = Sequential([\n",
    "    Embedding(10000, 128, input_length=max_len),\n",
    "    LSTM(64),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Fit model\n",
    "history = model.fit(train_data, train_data['label'], epochs=10, validation_data=(test_data, test_data['label']))\n",
    "\n",
    "# Evaluate model\n",
    "loss, accuracy = model.evaluate(test_data, test_data['label'])\n",
    "print(f'Test loss: {loss:.4f}')\n",
    "print(f'Test accuracy: {accuracy:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

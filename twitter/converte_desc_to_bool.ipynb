{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "import emoji\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "from Twitterpreprocessor import TwitterPreprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"./Data/data+descvf.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "profile_description  am bot that tweets about sports and technology follow me for updates\n",
      "doc  am bot that tweets about sports and technology follow me for updates\n",
      "[' ', 'bot', 'tweet', 'sport', 'technology', 'follow', 'update']\n",
      "pos count Counter({'NOUN': 4, 'PRON': 2, 'VERB': 2, 'ADP': 2, 'SPACE': 1, 'AUX': 1, 'CCONJ': 1})\n",
      "Profile description: I am a bot that tweets about sports and technology. Follow me for updates!\n",
      "Is bot? False\n"
     ]
    }
   ],
   "source": [
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "stop_words = set(stopwords.words('english'))\n",
    "def is_bot_profile(profile_description):\n",
    "    # remove emojis\n",
    "    profile_description = TwitterPreprocessor(str(profile_description)).desc_preprocess().text\n",
    "    print(\"profile_description\",profile_description)\n",
    "    # tokenize the text with spaCy\n",
    "    doc = nlp(profile_description)\n",
    "    print(\"doc\",doc)\n",
    "    # remove stop words and punctuation\n",
    "    tokens = [token.lemma_ for token in doc if not token.is_punct and not token.is_stop and token.lemma_ not in stop_words]\n",
    "    print(tokens)\n",
    "    # count the frequency of different parts-of-speech\n",
    "    pos_tags = [token.pos_ for token in doc]\n",
    "    pos_counts = Counter(pos_tags)\n",
    "    print(\"pos count\",pos_counts)\n",
    "    # look for patterns typical of bot accounts\n",
    "    if pos_counts['PRON'] >= 3 and pos_counts['NOUN'] >= 3:\n",
    "        return True\n",
    "    elif pos_counts['VERB'] >= 2 and pos_counts['ADJ'] >= 2:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "#  \"I am a bot that tweets about sports and technology. Follow me for updates!\"\n",
    "profile_description =\"I am a bot that tweets about sports and technology. Follow me for updates!\"\n",
    "is_bot = is_bot_profile(profile_description)\n",
    "print(f\"Profile description: {profile_description}\")\n",
    "print(f\"Is bot? {is_bot}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i am a bot that tweets about sports and technology. follow me for updates!\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'spacy.morphology.Morphology' object has no attribute 'tag_map'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 29\u001b[0m\n\u001b[0;32m     27\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m     28\u001b[0m profile_description \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mI am a bot that tweets about sports and technology. Follow me for updates!\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m---> 29\u001b[0m is_bot \u001b[39m=\u001b[39m is_bot_profile(profile_description)\n\u001b[0;32m     30\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mProfile description: \u001b[39m\u001b[39m{\u001b[39;00mprofile_description\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     31\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mIs bot? \u001b[39m\u001b[39m{\u001b[39;00mis_bot\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[1], line 19\u001b[0m, in \u001b[0;36mis_bot_profile\u001b[1;34m(profile_description)\u001b[0m\n\u001b[0;32m     17\u001b[0m tokens \u001b[39m=\u001b[39m [token\u001b[39m.\u001b[39mlemma_ \u001b[39mfor\u001b[39;00m token \u001b[39min\u001b[39;00m doc \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m token\u001b[39m.\u001b[39mis_punct \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m token\u001b[39m.\u001b[39mis_stop \u001b[39mand\u001b[39;00m token\u001b[39m.\u001b[39mlemma_ \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m stop_words]\n\u001b[0;32m     18\u001b[0m \u001b[39m# count the frequency of different parts-of-speech\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m pos_tags \u001b[39m=\u001b[39m [nlp\u001b[39m.\u001b[39mvocab\u001b[39m.\u001b[39mmorphology\u001b[39m.\u001b[39mtag_map[token\u001b[39m.\u001b[39mtag_][\u001b[39m'\u001b[39m\u001b[39mPOS\u001b[39m\u001b[39m'\u001b[39m] \u001b[39mfor\u001b[39;00m token \u001b[39min\u001b[39;00m nlp(\u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(tokens))]\n\u001b[0;32m     20\u001b[0m pos_counts \u001b[39m=\u001b[39m Counter(pos_tags)\n\u001b[0;32m     21\u001b[0m \u001b[39m# look for patterns typical of bot accounts\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[1], line 19\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     17\u001b[0m tokens \u001b[39m=\u001b[39m [token\u001b[39m.\u001b[39mlemma_ \u001b[39mfor\u001b[39;00m token \u001b[39min\u001b[39;00m doc \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m token\u001b[39m.\u001b[39mis_punct \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m token\u001b[39m.\u001b[39mis_stop \u001b[39mand\u001b[39;00m token\u001b[39m.\u001b[39mlemma_ \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m stop_words]\n\u001b[0;32m     18\u001b[0m \u001b[39m# count the frequency of different parts-of-speech\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m pos_tags \u001b[39m=\u001b[39m [nlp\u001b[39m.\u001b[39;49mvocab\u001b[39m.\u001b[39;49mmorphology\u001b[39m.\u001b[39;49mtag_map[token\u001b[39m.\u001b[39mtag_][\u001b[39m'\u001b[39m\u001b[39mPOS\u001b[39m\u001b[39m'\u001b[39m] \u001b[39mfor\u001b[39;00m token \u001b[39min\u001b[39;00m nlp(\u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(tokens))]\n\u001b[0;32m     20\u001b[0m pos_counts \u001b[39m=\u001b[39m Counter(pos_tags)\n\u001b[0;32m     21\u001b[0m \u001b[39m# look for patterns typical of bot accounts\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'spacy.morphology.Morphology' object has no attribute 'tag_map'"
     ]
    }
   ],
   "source": [
    "# import spacy\n",
    "# import emoji\n",
    "# from nltk.corpus import stopwords\n",
    "# from collections import Counter\n",
    "\n",
    "# nlp = spacy.load(\"en_core_web_sm\")\n",
    "# stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# def is_bot_profile(profile_description):\n",
    "#     # remove emojis\n",
    "#     # profile_description = emoji.get_emoji_regexp().sub(r'', profile_description)\n",
    "#     # lowercase the text\n",
    "#     profile_description = profile_description.lower()\n",
    "#     # tokenize the text with spaCy\n",
    "#     doc = nlp(profile_description)\n",
    "#     print(doc)\n",
    "#     # remove stop words and punctuation\n",
    "#     tokens = [token.lemma_ for token in doc if not token.is_punct and not token.is_stop and token.lemma_ not in stop_words]\n",
    "#     print(tokens)\n",
    "#     # count the frequency of different parts-of-speech\n",
    "#     pos_tags = [token.pos_ for token in doc]\n",
    "#     print(pos_tags)\n",
    "#     pos_counts = Counter(pos_tags)\n",
    "#     # look for patterns typical of bot accounts\n",
    "#     if pos_counts['PRON'] >= 3 and pos_counts['NOUN'] >= 3:\n",
    "#         return True\n",
    "#     elif pos_counts['VERB'] >= 2 and pos_counts['ADJ'] >= 2:\n",
    "#         return True\n",
    "#     else:\n",
    "#         return False\n",
    "# profile_description = \"I am a bot that tweets about sports and technology. Follow me for updates!\"\n",
    "# is_bot = is_bot_profile(profile_description)\n",
    "# print(f\"Profile description: {profile_description}\")\n",
    "# print(f\"Is bot? {is_bot}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Chams\\AppData\\Local\\Temp\\ipykernel_16676\\774891700.py:7: DtypeWarning: Columns (0,3,4,14,15) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  dataset = pd.read_csv(\"./Data/datafromAPI.csv\")\n"
     ]
    }
   ],
   "source": [
    "# import nltk\n",
    "# import spacy\n",
    "\n",
    "# from nltk.tokenize import word_tokenize\n",
    "\n",
    "# # Load the dataset\n",
    "# dataset = pd.read_csv(\"./Data/datafromAPI.csv\")\n",
    "\n",
    "# # Clean the text data\n",
    "# dataset['description'] = dataset['description'].apply(lambda x :TwitterPreprocessor(str(x)).desc_preprocess().text)\n",
    "# dataset['description'] = dataset['description'].fillna('')\n",
    "# # Define function to lemmatize the tokens\n",
    "# nlp = spacy.load('en_core_web_sm')\n",
    "# def lemmatize_tokens(tokens):\n",
    "#     doc = nlp(\" \".join(tokens))\n",
    "#     return [token.lemma_ for token in doc]\n",
    "\n",
    "# # Tokenize and lemmatize the text data\n",
    "# dataset['description_tokens'] = dataset['description'].apply(lambda x: lemmatize_tokens(word_tokenize(x)))\n",
    "\n",
    "# # Remove stop words\n",
    "# stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "# dataset['description_tokens'] = dataset['description_tokens'].apply(lambda x: [word for word in x if word not in stop_words])\n",
    "# dataset.to_csv('./Data/description_tokens.csv', index=False)\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# # Convert the list of tokens to a string\n",
    "# dataset['description_tokens'] = dataset['description_tokens'].apply(lambda x: ' '.join(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Add the dense array to the dataset\n",
    "# dataset['description_tokens'] = X_array.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset.to_csv('./Data/description_tokensv2.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "start prepare desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import re\n",
    "# from nltk.corpus import stopwords\n",
    "# from nltk.stem import PorterStemmer\n",
    "\n",
    "# # Load dataset\n",
    "# df = pd.read_csv('./Data/data+descvf.csv')\n",
    "\n",
    "# # Remove irrelevant or duplicated data\n",
    "# df = df.drop(['description',  \n",
    "#          'account_type'], axis=1)\n",
    "\n",
    "# # Remove special characters and punctuation, and convert text to lowercase\n",
    "# df['description'] = df['description'].apply(lambda x :TwitterPreprocessor(str(x)).desc_preprocess().text)\n",
    "\n",
    "# # Tokenize text\n",
    "# df['description'] = df['description'].apply(lambda x: x.split())\n",
    "\n",
    "# # Remove stop words\n",
    "# stop_words = set(stopwords.words('english'))\n",
    "# df['description'] = df['description'].apply(lambda x: [word for word in x if word not in stop_words])\n",
    "\n",
    "# # Stemming\n",
    "# stemmer = PorterStemmer()\n",
    "# df['description'] = df['description'].apply(lambda x: [stemmer.stem(word) for word in x])\n",
    "\n",
    "# # Convert back to string\n",
    "# df['description'] = df['description'].apply(lambda x: ' '.join(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import tensorflow as tf\n",
    "# from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "# from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "# from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "# from tensorflow.keras.models import Sequential\n",
    "# # Split into training and testing sets\n",
    "# train_size = int(0.8 * len(df))\n",
    "# train_data = df[:train_size]\n",
    "# test_data = df[train_size:]\n",
    "\n",
    "# # Tokenize text inputs\n",
    "# tokenizer = Tokenizer(num_words=10000, oov_token='<OOV>')\n",
    "# tokenizer.fit_on_texts(train_data['description'])\n",
    "\n",
    "# # Convert text inputs to sequences of integers\n",
    "# train_sequences = tokenizer.texts_to_sequences(train_data['description'])\n",
    "# test_sequences = tokenizer.texts_to_sequences(test_data['description'])\n",
    "\n",
    "# # Pad sequences\n",
    "# max_len = 100\n",
    "# train_data = pad_sequences(train_sequences, maxlen=max_len, padding='post', truncating='post')\n",
    "# test_data = pad_sequences(test_sequences, maxlen=max_len, padding='post', truncating='post')\n",
    "\n",
    "# # Build model\n",
    "# model = Sequential([\n",
    "#     Embedding(10000, 128, input_length=max_len),\n",
    "#     LSTM(64),\n",
    "#     Dense(64, activation='relu'),\n",
    "#     Dropout(0.5),\n",
    "#     Dense(1, activation='sigmoid')\n",
    "# ])\n",
    "\n",
    "# # Compile model\n",
    "# model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# # Fit model\n",
    "# history = model.fit(train_data, train_data['label'], epochs=10, validation_data=(test_data, test_data['label']))\n",
    "\n",
    "# # Evaluate model\n",
    "# loss, accuracy = model.evaluate(test_data, test_data['label'])\n",
    "# print(f'Test loss: {loss:.4f}')\n",
    "# print(f'Test accuracy: {accuracy:.4f}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"Key 'zbb' not present\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 62\u001b[0m\n\u001b[1;32m     60\u001b[0m description_vectors \u001b[39m=\u001b[39m []\n\u001b[1;32m     61\u001b[0m \u001b[39mfor\u001b[39;00m lemmatized_tokens \u001b[39min\u001b[39;00m lemmatized_tokens_list:\n\u001b[0;32m---> 62\u001b[0m     description_vector \u001b[39m=\u001b[39m \u001b[39msum\u001b[39;49m(model\u001b[39m.\u001b[39;49mwv[token] \u001b[39mfor\u001b[39;49;00m token \u001b[39min\u001b[39;49;00m lemmatized_tokens) \u001b[39m/\u001b[39m \u001b[39mlen\u001b[39m(lemmatized_tokens)\n\u001b[1;32m     63\u001b[0m     description_vectors\u001b[39m.\u001b[39mappend(description_vector)\n\u001b[1;32m     65\u001b[0m \u001b[39m# print vector representation of each description\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[8], line 62\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     60\u001b[0m description_vectors \u001b[39m=\u001b[39m []\n\u001b[1;32m     61\u001b[0m \u001b[39mfor\u001b[39;00m lemmatized_tokens \u001b[39min\u001b[39;00m lemmatized_tokens_list:\n\u001b[0;32m---> 62\u001b[0m     description_vector \u001b[39m=\u001b[39m \u001b[39msum\u001b[39m(model\u001b[39m.\u001b[39;49mwv[token] \u001b[39mfor\u001b[39;00m token \u001b[39min\u001b[39;00m lemmatized_tokens) \u001b[39m/\u001b[39m \u001b[39mlen\u001b[39m(lemmatized_tokens)\n\u001b[1;32m     63\u001b[0m     description_vectors\u001b[39m.\u001b[39mappend(description_vector)\n\u001b[1;32m     65\u001b[0m \u001b[39m# print vector representation of each description\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/AI/venv/lib/python3.10/site-packages/gensim/models/keyedvectors.py:403\u001b[0m, in \u001b[0;36mKeyedVectors.__getitem__\u001b[0;34m(self, key_or_keys)\u001b[0m\n\u001b[1;32m    389\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Get vector representation of `key_or_keys`.\u001b[39;00m\n\u001b[1;32m    390\u001b[0m \n\u001b[1;32m    391\u001b[0m \u001b[39mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    400\u001b[0m \n\u001b[1;32m    401\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    402\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(key_or_keys, _KEY_TYPES):\n\u001b[0;32m--> 403\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_vector(key_or_keys)\n\u001b[1;32m    405\u001b[0m \u001b[39mreturn\u001b[39;00m vstack([\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_vector(key) \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m key_or_keys])\n",
      "File \u001b[0;32m~/Documents/AI/venv/lib/python3.10/site-packages/gensim/models/keyedvectors.py:446\u001b[0m, in \u001b[0;36mKeyedVectors.get_vector\u001b[0;34m(self, key, norm)\u001b[0m\n\u001b[1;32m    422\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_vector\u001b[39m(\u001b[39mself\u001b[39m, key, norm\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m    423\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Get the key's vector, as a 1D numpy array.\u001b[39;00m\n\u001b[1;32m    424\u001b[0m \n\u001b[1;32m    425\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    444\u001b[0m \n\u001b[1;32m    445\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 446\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_index(key)\n\u001b[1;32m    447\u001b[0m     \u001b[39mif\u001b[39;00m norm:\n\u001b[1;32m    448\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfill_norms()\n",
      "File \u001b[0;32m~/Documents/AI/venv/lib/python3.10/site-packages/gensim/models/keyedvectors.py:420\u001b[0m, in \u001b[0;36mKeyedVectors.get_index\u001b[0;34m(self, key, default)\u001b[0m\n\u001b[1;32m    418\u001b[0m     \u001b[39mreturn\u001b[39;00m default\n\u001b[1;32m    419\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 420\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mKey \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mkey\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m not present\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"Key 'zbb' not present\""
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from gensim.models import Word2Vec\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "from Twitterpreprocessor import TwitterPreprocessor\n",
    "\n",
    "# download required NLTK data\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('wordnet')\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv('./Data/data+descvf.csv')\n",
    "\n",
    "\n",
    "# Remove special characters and punctuation, and convert text to lowercase\n",
    "df['description'] = df['description'].apply(lambda x :TwitterPreprocessor(str(x)).desc_preprocess().text)\n",
    "df = df.dropna(subset=['description'])\n",
    "# example list of string descriptions\n",
    "descriptions = df['description'] \n",
    "# create list of stopwords to remove\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# create WordNet lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# initialize list of lemmatized tokens for each description\n",
    "lemmatized_tokens_list = []\n",
    "\n",
    "# loop through each description, performing POS tagging, stopword removal, and lemmatization\n",
    "for description in descriptions:\n",
    "    # perform tokenization\n",
    "    tokens = word_tokenize(description.lower())\n",
    "\n",
    "    # perform POS tagging\n",
    "    pos_tags = nltk.pos_tag(tokens)\n",
    "\n",
    "    # initialize list of lemmatized tokens for this description\n",
    "    lemmatized_tokens = []\n",
    "\n",
    "    # loop through POS tagged tokens, removing certain parts of speech and lemmatizing others\n",
    "    for token, pos in pos_tags:\n",
    "        if pos.startswith('N') or pos.startswith('J'):\n",
    "            # keep nouns and adjectives, and lemmatize them\n",
    "            lemma = lemmatizer.lemmatize(token)\n",
    "            if lemma not in stop_words:\n",
    "                lemmatized_tokens.append(lemma)\n",
    "\n",
    "    # add lemmatized tokens for this description to list\n",
    "    lemmatized_tokens_list.append(lemmatized_tokens)\n",
    "\n",
    "# train Word2Vec model on all lemmatized tokens\n",
    "model = Word2Vec(lemmatized_tokens_list, min_count=5)\n",
    "\n",
    "# get vector representation of each description by averaging the vectors of its constituent lemmatized tokens\n",
    "description_vectors = []\n",
    "for lemmatized_tokens in lemmatized_tokens_list:\n",
    "    description_vector = sum(model.wv[token] for token in lemmatized_tokens) / len(lemmatized_tokens)\n",
    "    description_vectors.append(description_vector)\n",
    "\n",
    "# print vector representation of each description\n",
    "for i, vector in enumerate(description_vectors):\n",
    "    print(f\"Description {i+1} vector: {vector}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from gensim.models import Word2Vec\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "from Twitterpreprocessor import TwitterPreprocessor\n",
    "df = pd.read_csv('./Data/data+descvf.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['arabic', 'azerbaijani', 'basque', 'bengali', 'catalan', 'chinese', 'danish', 'dutch', 'english', 'finnish', 'french', 'german', 'greek', 'hebrew', 'hinglish', 'hungarian', 'indonesian', 'italian', 'kazakh', 'nepali', 'norwegian', 'portuguese', 'romanian', 'russian', 'slovene', 'spanish', 'swedish', 'tajik', 'turkish']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "print(stopwords.fileids())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['He', 'determined', 'drop', 'litigation', 'monastry', ',', 'relinguish', 'claims', 'wood-cuting', 'fishery', 'rihgts', '.', 'He', 'ready', 'becuase', 'rights', 'become', 'much', 'less', 'valuable', ',', 'indeed', 'vaguest', 'idea', 'wood', 'river', 'question', '.']\n",
      "['he', 'determin', 'drop', 'litig', 'monastri', ',', 'relinguish', 'claim', 'wood-cut', 'fisheri', 'rihgt', '.', 'he', 'readi', 'becuas', 'right', 'becom', 'much', 'less', 'valuabl', ',', 'inde', 'vaguest', 'idea', 'wood', 'river', 'question', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "set(stopwords.words('english'))\n",
    "\n",
    "text = \"\"\"He determined to drop his litigation with the monastry, and relinguish his claims to the wood-cuting and \n",
    "fishery rihgts at once. He was the more ready to do this becuase the rights had become much less valuable, and he had \n",
    "indeed the vaguest idea where the wood and river in question were.\"\"\"\n",
    "\n",
    "stop_words = set(stopwords.words('english')) \n",
    "  \n",
    "word_tokens = word_tokenize(text) \n",
    "    \n",
    "filtered_sentence = [] \n",
    "  \n",
    "for w in word_tokens: \n",
    "    if w not in stop_words: \n",
    "        filtered_sentence.append(w) \n",
    "\n",
    "Stem_words = []\n",
    "ps =PorterStemmer()\n",
    "for w in filtered_sentence:\n",
    "    rootWord=ps.stem(w)\n",
    "    Stem_words.append(rootWord)\n",
    "print(filtered_sentence)\n",
    "print(Stem_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Language.ENGLISH"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from lingua import Language, LanguageDetectorBuilder\n",
    "languages = [Language.ENGLISH, Language.FRENCH, Language.GERMAN, Language.SPANISH]\n",
    "detector = LanguageDetectorBuilder.from_languages(*languages).build()\n",
    "C=detector.detect_language_of(\"languages are awesome\")\n",
    "C.tolower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import gensim\n",
    "from gensim.models import word2vec\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fasttext-wiki-news-subwords-300', 'conceptnet-numberbatch-17-06-300', 'word2vec-ruscorpora-300', 'word2vec-google-news-300', 'glove-wiki-gigaword-50', 'glove-wiki-gigaword-100', 'glove-wiki-gigaword-200', 'glove-wiki-gigaword-300', 'glove-twitter-25', 'glove-twitter-50', 'glove-twitter-100', 'glove-twitter-200', '__testing_word2vec-matrix-synopsis']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import gensim.downloader as api\n",
    "print(list(gensim.downloader.info()['models'].keys()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0b269af9bc916de9a69a9129747e30eb936050c74c311d5baf33ccd4b86d3e37"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
